{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"z_RQDzfUeZjG","outputId":"e16f19b8-0d9b-4ae9-e0c8-77aef17318ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/Laptop/0a.Nathan Model\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"]}],"source":["#tmerninal this to kill the python scripts\n","#pkill -9 python\n","\n","# ============================\n","# STEP 1: MOUNT GOOGLE DRIVE\n","# ============================\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","\n","# Change directory to your project\n","PROJECT_PATH = '/content/drive/Othercomputers/Laptop/0a.Nathan Model'\n","%cd \"$PROJECT_PATH\"\n","\n","# ============================\n","# STEP 2: INSTALL DEPENDENCIES\n","# ============================\n","# Install requirements if you have a requirements.txt file\n","!pip install -qr \"colab_requirements.txt\"\n","#!pip install -q pyarrow==14.0.2 transformers[torch] datasets evaluate\n","\n","# Check if torch is using GPU\n","import torch\n","print(\"GPU Available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","\n","# ============================\n","# STEP 3: RUN YOUR NLP CODE\n","# ============================\n","print(\"Running model...\")\n","!python start.py\n","#testing basic og function, tf rels = False => 87,71,74 full\n","#then add neg sampling before binary filter, tf rels = False => 87,70,72 3000   (74.35, 57.78, 65.03)\n","#span neg sampling and add rel neg sampling, tf rels = False => 88, 69, 71 => rel pr is low => I think if you neg sample the rels, you must teacher force the rels also\n","#span neg sampling, rel neg sampling, add prune rels after rel neg sampling to save space, tf rels = False  => this crashes, need to fix the rel rep code\n","#span neg sampling, rel neg sampling, tf rels = True   88,71,73   3000\n","\n","#from what I see neg sampling ont eh spans only works with no tf on teh rels and no neg sampling on the rels\n","\n","# BE tagger => span neg sampling => span filter with tagger scores => rel gen   86,68,70 3000  => uses less memory\n","# span neg sampling => BE tagger => span filter with tagger scores => rel gen   86,66,67 3000\n","# BE tagger => span neg sampling => span filter with tths scores => rel-neg-sampling => rel gen  87,70,72 3000\n","# BECO tagger => span neg sampling => span filter with tagger scores => rel-neg-sampling => rel gen 87,69,69 3000\n","# BE tagger => span neg sampling => span filter with tths scores => rel-neg-sampling => tf rels => rel gen  86,70,70 3000 => takes longer to train but will get there...\n","# BE tagger => span filter with tths scores => rel gen\n","\n","# BE tagger => span filter with tths scores => span marker => rel gen   => 85,70,70 => so marking doesn't really help at all, go figure....\n","\n","\n","#approximating unicausla pair classification\n","#so to simulate unicausal we have to use only the end classifier losses and TF for eval, for real conditions, we need all the losses and no TF for eval and th etraining limits to arodun 40%\n","#it like neg sampling through, seems to be beneficial to model training in all scenarios, delays overfitting and gives better control of P/R\n","\n","#separating berts for span and rel may help a bit, but do not really change the fundamental issues\n","\n","\n","#basicalyl using loose matching helps the F1 for spans get up to arodun 75 for spans and 50 for rels, but the results are somewhat unstable\n","#this is somewhat interesting though, at least for the spans, the rels are still quite garbage though, obviously there is more to the causal stuff\n","#using token tagging seems to give slightly better F1 accross the board, but hte lost rels in teh eval are more than the binary filtering, weird, maybe it doesn't matter if the matching is loose!!!\n","#add cls to rel reps\n","\n","\n","#try removing cls and width embeddings from span reps\n","#try changing to FFN projection\n","\n","\n","#teh cosine loss did not have much impact, it is not able to train the model by iteself, the signal is too random.\n","#If you mix it wiht the regular loss, it works ok, but it works better without it\n","#need to test it on long spans"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}