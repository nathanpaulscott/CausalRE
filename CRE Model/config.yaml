#####################################################################
#Run Config
#####################################################################
run_type: train                        #train/predict.  
predict_split: test                       #the name of the split to use for predictions/inference
predict_folder: preds
#NOTE:
#For train you need 3 labeled splits in the data json file (train/val/test) each split needs the tokens key along with the spans and relations keys for the labels. 
#'predict' is unlabelled inference, you need one unlabelled split in the data json file (split is 'predict' and only keys are 'tokens').
random_seed: 42
#display
eval_step_display: False                     #display the preds for the first batch obs in eval batch idx
log_step_data: False                    #show the training step data in the logs (not really needed, just log the eval points)
loss_breakdown: True
collect_grads: False                    #true to collect and map gradients for debugging
#####################################################################
#Directory Paths
#####################################################################
model_folder: "models"                  #location of the models folder
model_name: "test"                      #pretrained model name => Use "none" to not load a model
model_save: True
model_load: False
#--------------------------------------------------------------------------------
log_folder: logs                        #location of the log file folder
print_log: True                         #print the log to screen as it is written
#---------------------------------------
#data_path: "data/sample/sample data.json"      #the dataset file
#data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#---------------------------------------
#data_path: "data/final/conll04_nathan.json"      #the dataset file
#data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#---------------------------------------
#data_path: "data/final/conll04_nathan_for_model_rs129.json"      #the dataset file
#data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#---------------------------------------
#data_path: "data/unicausal/model_data_altlex.json"      #the dataset file  => span width max is 116, seq len 135
#data_format: id                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#NOTE: the annotation format for this dataset is not rational, the typically split each sentence into 2 parts with a causal trigger word in between if at all, there seems to not a much reason for the annotations, it is difficiult to extract a model from this
#---------------------------------------
#data_path: "data/unicausal/model_data_because.json"      #the dataset file, span width max is 56, seq len 99
#data_format: id                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#there is some issue with this data set, teh support is only 38 for the eval set for spans?!!?!? and 14 for rels, not right
#---------------------------------------
#data_path: "data/unicausal/model_data_ctb.json"      #the dataset file, span width max = 7, seq len max = 130
#data_format: id                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#---------------------------------------
#data_path: "data/final/model_data_semeval.json"      #the dataset file, max seq len = 99, max span width = 4, Span F1 87%, Rel F1 67%    (regular:83,63, attn-crossattn: 82, 63)
#data_format: id                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#NOTE: this dataset gets good performance, it has a large number ofobservations (8000+), with about 90% non causal.  the causal spans are typically entity form single word triggers for events or states and the causal prelation is explicit.  So these are a very specific form of causal relation in text
#---------------------------------------
#data_path: "data/new causal processed mar25/maven/maven - data - short.json"      #the dataset file, max seq len = 86, max span width = 44 => won't really train, spans get up to arodun 40% F1, rels are unstable 0-10%
#data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#NOTE: this had the raw sentences annotated by me and the llm, needs a lot of clean up, doesn't work so well
#---------------------------------------
#data_path: "data/new causal processed mar25/maven/old/maven - data - short - triggers_for_annotation_for_model.json"      #the dataset file, max seq len = 99, max span width = 4      (span F1 80+, rel F1 40+)  So spans are good because it is short, rels are average though
#data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#This only contains the triggers, mostly verbal, not human checked, straight out of gemini
#---------------------------------------
#data_path: "data/unicausal/mixed_754_for_model.json"      #the dataset file  => span width max is 116, seq len 135   (61, 46)
#data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx
#this is a combination dataset of altlex, bits from maven and semeval and some synthetic data, about 70-80% causal, annoatted with long spans by me, gets pretty average performance due to long span noise and convoluted linguistic structure
#---------------------------------------
#data_path: "data/unicausal/mixed_triggers_297_for_model.json"      #span F1 62%, rel F1 39%
#data_format: idx                         
#NOTE: this is the same custom dataset with pruned spans down to the kernel of the event/state, but I leave adjacent words in if they enhance the span meaning, so not pure triggers
#---------------------------------------
data_path: "data/final/mixed_final_for_model_80-10-10_rs129.json" 
data_format: idx                         #id/idx.  'id' is when the spans and rels have a unique id and the head and tail of the rel reference the span id.  idx is the spert format where there is no id and the rel head/tail reference the span list idx



#####################################################################
#General Configs
#####################################################################
predict_thd: 0.3     #0.3               #used in the multilabel case, for the final class predictions, applied to the sigmoid  of each class logits, range is > 0 and < 1, lower => easy to predict a class, higher => harder to predict a class.  NOTE: DONT set to 0 or 1
predict_conf: False                     #the predict confidence
rel_labels: multilabel                  #unilabel/multilabel => unilabel = 1 label per rel, uses CELoss.  multilabel = multiple labels per rel, uses BCELoss
eval_type: manual                       #skl/manual => whether to use skl metrics, which might be slower, but allows changing the averaging type, otherwise it is locked to micro
f1_ave: micro                           #micro/macro...  for the metrics
#metrics matching
matching_loose: True                    #use loose span matching or exact
matching_tolerance: 0.15                 #span width tolerance for loose matching.  >= 1 => absolute, i.e. +- that many words on each side. <1 is % of each spans width, i.e. 0.15 => for a span of width 10, the tolerance is 1.5 words on each side (so basically 1)
matching_width_limit: 8                 #span width min limit for loose matching, i.e. do not apply to smaller spans than this
matching_make_binary: True              #treat labels and preds as binary, only applied for loose matching == True

#####################################################################
#Model Configuration
#####################################################################
#backbone_model_name: bert-base-uncased  #name of the flair or hf model, eg 'microsoft/deberta-v3-base' or 'bert-base-uncased'
#backbone_model_name: "/content/drive/Othercomputers/Laptop/0a.Nathan Model/backbone/bert-base-cased"     #use a predownloaded version as HF servers suck
backbone_model_name: "/content/drive/Othercomputers/Laptop/0a.Nathan Model/backbone/spanbert-base-cased"     #use a predownloaded version as HF servers suck

backbone_model_source: HF               #HF, only HF is now supported, flair wa snot so useful
###############
subtoken_pooling: maxpool               #first/last/first_last/mean/maxpool/none (maxpool and none are HF only).  none will disable subtoken pooling, only supported by HF.  NOTE: first_last will double the hidden dim, but the projection layer will pull it back to normal
shuffle_train: True                     #always be true, except for debugging
freeze_encoder: False                   #whether to allow fine tuning of the transformer encoder params (eg. bert), true will freeze the encoder and not allow training 
bert_shared_unmarked_span_rel: True     #whether to use a separate bert for spans and rels or share it
hidden_size: 768                        #hidden size of the bert based encoder
dropout: 0.1      #0.1                      #standard model dropout
max_seq_len: 200  #200                  #word token limit, this will generally affect the model speed, so keep it as low as possible.  I have this in just a  a protection fro very long sequences coming in
max_span_width: 80   #80                #max span width in word tokens (10-12 for NER, 40+ for long span event detection)
dump_missed_spans_on_import: True       #ture/false: true => dumps the annotated spans to the log file that could not be used due to the max_seq_len and max_span_width limits
num_precision: full                     #full/half => the num precision to use for the model => full => 32bit, half=>16bit
projection_layer_type: ffn   #ffn           #simple/ffn (ffn uses too much memory). for the span_rep and rel_rep generation output layers.  'simple' to just use a simple projection layer for the output or a 'ffn' for a more complex ffn layer.  Not implemented yet, just use 'ffn'
ffn_ratio: 4                            #FFN hidden expansion factor if projection layer is ffn
use_lstm: True   #True                          #whether to use an lstm layer after hte transformer encoder, this was in graphER, not sure if is of any benefit
lstm_layers: 3
lstm_skip_con: True

#token tagger configs
token_tagger: True                     #true/false => true to use the token tagger for span preds, tagger loss and filter scores, false to bypass.
tagging_mode: BE                        #BE/BECO => BE is Begin-End, multiclass, BECO = Begin-End-Combine-Other => I do see better perf on rels with BECO

###############
#span rep config
###############
width_embedding_size: 100             #hidden size of the span width embedding, spert had 2
span_mode: nathan                      #firstlast_grapher/firstlast/spert/nathan/attn... used to configure the method used to generate the span reps from the token reps, NEED TO EXPLAIN WHAT EACH ONE DOES   attn needs lots of memory so have to make batch size 1 and clear tensors each batch
span_win_alpha: 0.2   #0.2            #the alpha used to determine the start/end token windows as a % of the span_width (round(span_width*alpha) for nathan's method, use 1 to disable this and just use the first and last tokens.
use_width_embedding: True
use_cls_embedding: True

###############
#span/rel filtering params              #these are from graphER for choosing the spans per obs for the initial graph, I will keep them for now, but will probably change the calc later
###############
#NOTE: a key point here is limiting the number of spans for the initial graph as the number of rels is quadratically dependent on this number
#This is basically a smart form of neg sampling
#the top_k_spans/rels is basically the min of the num_spans/rels, the limit (max_top_k_spans/rels) and a dynamic limit based on the span/rel score precentile
span_filtering_type: tths               #tths/bfhs/both => 
                                        #tths uses the token tagging head scores to prune the spans to top_k_spans based on the token tagger filter scores, 
                                        #bfhs uses the scores from the binary filter head to prune spans to top_k_spans from the bfh filter scores, 
                                        #both filters the spans to top_k_spans_pre using the token tagger filter scores, then processes this in the binary filter head and filters down to top_k_spans with the bfh filter scores
max_top_k_spans_pre: 1000               #max number of spans to shortlist for stage 1 of the 2 stage case where we first filter on the token tagger filter scores before going through the binary filter head
max_top_k_spans: 30  #50                     #max number of spans to shortlist for the initial graph
max_top_k_rels: 200   #200                  #max number of rels to choose per obs for the initial graph, if the number available goes below this, then that is the limit.  This is the hard limit only overridden by the percentile filter
filter_dropout: True                    #whether to use dropout before the binary filter head
filter_head_type: single                #single/double => whether to use a single class binary filter head or double class binary filter head for the span/rel filtering, both work in similar ways, double was used in graphER
overlap_thd: 0.8   #0.8                 #range (0 to 1) spans with this much overlap are considered conflicted (one is potentially redundant), this is a normalised overlap metric so it ranges from 0 = no overlap to 1 = identical
consistency_penalties_train: True          #1 works best....#none/1/2/3 => 3 uses both type 1 an type 2 penalties, 'none' has no penalties, '1' will identify redundant spans first then hanging relations after, the penatly for redundant spans is tied to the logit of that span and the hanging rel is the logit of that rel. '2' the hanging rel penalty is based on the missing span logit and the redundant span penalty is the logit of teh redundant span that is not a label or in a pos predicted relation.
post_model_prune_inference: True           #1 works best.....#none/1/2 => 'none' has no pruning, '1' will prune redundant spans first then remove hanging relations after. '2' will add missing spans from hanging rels and use a forced logit and then only remove redundant spans if they are not in relations.
penalty_step_limit: 3000                #when to start doing this for train
##############
#lost rel handling
##############
lost_rel_alpha: 0.5   #0.5                       #0 to inf, global adjustment of the lost rel penalty to get the lost rel loss => higher penalises the model more for missing spans causing lost rels (in non-teacher forced mode)
redundant_span_alpha: 1   #1
hanging_rel_alpha: 1    #1
###############
#rel rep config
###############
rel_mode: between_window_context       #no_context/between_context/window_context/between_window_context => add context reps in (no context, from tokens between spans, from tokens in windows before/after each span)  => only beteween works well
rel_no_context_rep: emb                #zero/emb => whether to replace the context reps in the case of no valid context tokens, to all zero (zero) or a learned no context embedding (emb)
rel_context_pooling: crossattn         #max/selfattn/crossattn/crossattn_pure => how to pool the valid context tokens to get one context rep.  max => max pool them, selfattn just use a pooler token to pool the context tokens then concat with head and tail reps, crossattn(pure) => use attention pooling w.r.t the concatenated and reprojected head and tail spans as the query (for_pure: no head and tail concat after this)
rel_window_size: 30   #15              #tokens or subtoken window size for the window algo
modified_span_reps_for_rel_reps: True  #whether to use just the span reps without the width embedding or cls embedding when building the rel reps, if False, it just reuses the span reps

###############
#graph embedding config
###############
graph_id_method: add                    #whether to add or prefix the node and edge identifiers onto the span and rel reps respectively

###############
#graph transformer config
###############
use_graph: True                         #True/False, if False, the whole graph and graph transformer is bypassed
use_graph_reps: True                    #True => use the node/edge reps output from the graph transformer, these will be enriched embeddings but also include the node/edge identifiers mixed in
                                        #False => use the pre-graph transformer span/rel reps, as output, only use the filtering scores and loss from the graph transformer
num_heads: 8                            #used by the graph transformer layer 
num_transformer_layers: 3               #used by the graph transformer layer
graph_skip_con: True

###############
#graph filtering params                 #these are to make it easier or harder to prune nodes/edges from the graph using the node/edge filter_scores
###############
graph_filter_head_type: single          #single/double => whether to use a single class binary filter head or double class binary filter head for the graph filtering, both work in similar ways, single was used in graphER

#####################################################################
#Training Parameters
#####################################################################
num_steps: 20000                        #how many batches to run through the system, the total batches in the dataset is going to be much smaller, so it just repeats the batches
eval_every: 100    #200                 #eval every 'eval_every' batches
model_min_save_steps: 4000 #4000
model_min_eval_steps: 1000
model_early_stopping: 25 #20          #if the score doesn't increase for this many eval runs, then exit train loop, 500 eval step, use 5, 250 eval step use 10
balance_reduction_factor: 2  #2           #(make it less than 1), closer to 0 to lessen the effect, closer to 1 to enhance the focus on balance between p and r
save_score_ema_alpha: 1  # 0.6             #smoothes the early stopping calc a bit
train_batch_size: 2  #4                 #num obs per train batch
accumulation_steps: 1 #1                #accumulate losses for this many batches, so effective batch size is accumulation_steps*train_batch_size
eval_batch_size: 4 #6                   #num obs per val/test batch
opt_type: 2                             #1/2 => 1 use the same lr and wd for all params, 2 => use specific lr/wds 
opt_weight_decay:  0.01                 #adamw optimiser weight decay
lr_encoder_span: 1e-5  #1e-5                 #this is the learning rate used for the tranformer encoder
lr_encoder_rel: 1e-5  #1e-5                 #this is the learning rate used for the tranformer encoder
lr_others: 5e-5   #5e-5                 #this is the learning rate used for all other weights other than the transformer encoder
warmup_ratio: 0.1                       #warmup_steps will thus be warmup_ratio * num_steps
scheduler_type: linear #'constant' #linear                  #'cosine', 'linear', 'constant', 'polynomial', 'inverse_sqrt'
loss_reduction: mean                    #mean/sum => whether to use loss reduction of sum or mean as well as whether to mean the accumulation losses or not before backprop => sum will have large grads, mean will have small grads.  Mean will average the losses per label
clear_tensor_steps: 10    #10           #clear the cuda cache and gc collect, it actually runs del tensors each step (which is fast)
grad_clip: 10     #1                    #max gradient clipping, linearly reduces down to 1 at num_steps/2
span_loss_mf: 1                         #mult the span loss by this much
rel_loss_mf: 1                          #mult the rel loss by this much
#####################################################################
#Advanced Training Settings
#####################################################################
#teacher forcing
#these params control teacher forcing in the spans/rel filter heads, the graph filter head, span output and rel output heads never use teacher forcing
#NOTE: the span filter head is more critical than the rel filter head as relation generation depends on it, thus we have to be more careful about turning off TF, so make sure the step limit is long enough for the model stabilize first
#it also ties into the lost_rel penalty algorithm, you need to make sure the penalty is sufficient to inform the model of it's errors so it learns
span_force_pos: temp                  #always-eval/always/temp/never: always-eval keeps TF on even for eval, used to test the performance of the realtion pipeline.  always => TF is always in training.  temp => TF is used for the sepcified number fo steps
force_pos_step_limit: 3000              #when to disable teacher forcing for the span/rel filter heads, set this so the model gets in the ballpark for span predictions before disabling it as it will spike lost rels
rel_force_pos: follow                   #follow/never, if follow, it will follow the span_teacher_forcing setting (always/temp), if never, then teacher forcing is always off for the rel filter head.  if span_force_pos = always-eval, and rel is follow, then rel will use always, but not be on for eval

#neg sampling
#NOTE: neg sampling is applied only during training to reduce the number of neg cases
span_neg_sampling_limit: 100             #max number of neg samples per batch obs
span_neg_sampling: False                 #True/False
rel_neg_sampling_limit: 100              #max number of neg samples per batch obs
rel_neg_sampling: False                  #True/False

